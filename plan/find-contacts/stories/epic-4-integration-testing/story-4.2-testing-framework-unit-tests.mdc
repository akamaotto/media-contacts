# Story 4.2: Testing Framework & Unit Tests
**Epic**: Epic 4: Integration & Testing
**Estimated Time**: 2 days
**Priority**: High
**Status**: Pending
**Assignee**: QA Engineer + Backend Developer
**GitHub Issue**: # (to be created)
**Labels**: epic-4, story, priority-high, find-contacts, testing

## GitHub CLI Issue Creation
```bash
gh issue create \
  --title "Story 4.2: Testing Framework & Unit Tests" \
  --body "$(cat <<'EOF'
## Epic
Epic 4: Integration & Testing

## Objective
Set up comprehensive testing infrastructure and achieve >90% unit test coverage for all AI search components and integration services.

## Acceptance Criteria
- [ ] Comprehensive testing framework is established and configured
- [ ] Unit test coverage exceeds 90% for all new AI search code
- [ ] Test utilities and helpers are created for common testing patterns
- [ ] Mock services are implemented for external AI APIs
- [ ] Test data fixtures are created for comprehensive test scenarios
- [ ] Continuous integration pipeline runs all tests automatically
- [ ] Test execution time is optimized for fast feedback
- [ ] All edge cases and error scenarios are covered by tests
- [ ] Tests are maintainable and follow established patterns
- [ ] Test documentation provides clear guidance for future development

## Technical Requirements
- Set up Jest configuration for AI search feature testing
- Create test utilities for API testing, database operations, and mocking
- Implement comprehensive unit tests for all backend services
- Create test utilities for frontend component testing
- Set up test database with proper seeding and cleanup
- Implement mock services for OpenAI, Anthropic, Exa, and Firecrawl APIs
- Create test fixtures for various search scenarios and edge cases

## Definition of Done
- All acceptance criteria are met and validated
- Unit test coverage >90% for all critical code paths
- Test framework is documented and team is trained
- CI/CD pipeline runs tests automatically
- Test execution time is optimized (<5 minutes for full suite)
- All tests are passing and maintainable

## Story File
[View Full Story Details](plan/find-contacts/stories/epic-4-integration-testing/story-4.2-testing-framework-unit-tests.mdc)

---
ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)
EOF
)" \
  --label "epic-4" \
  --label "story" \
  --label "priority-high" \
  --label "find-contacts" \
  --label "testing" \
  --label "backend" \
  --label "quality"
```

## Objective
Set up comprehensive testing infrastructure and achieve >90% unit test coverage for all AI search components and integration services. This story establishes the foundation for all subsequent testing work by creating the framework, utilities, and initial unit tests.

## Acceptance Criteria
- [ ] Comprehensive testing framework is established and configured
- [ ] Unit test coverage exceeds 90% for all new AI search code
- [ ] Test utilities and helpers are created for common testing patterns
- [ ] Mock services are implemented for external AI APIs
- [ ] Test data fixtures are created for comprehensive test scenarios
- [ ] Continuous integration pipeline runs all tests automatically
- [ ] Test execution time is optimized for fast feedback
- [ ] All edge cases and error scenarios are covered by tests
- [ ] Tests are maintainable and follow established patterns
- [ ] Test documentation provides clear guidance for future development

## Technical Requirements

### Core Features
- Set up Jest configuration for AI search feature testing
- Create test utilities for API testing, database operations, and mocking
- Implement comprehensive unit tests for all backend services
- Create test utilities for frontend component testing
- Set up test database with proper seeding and cleanup
- Implement mock services for external AI APIs

### Integration Points
- Existing Jest configuration in the project
- Prisma test database setup
- Existing testing patterns and utilities
- CI/CD pipeline configuration
- Code coverage reporting tools

### Dependencies
- Epic 1: Foundation & Infrastructure (API infrastructure ready for testing)
- Epic 2: AI Search Service (services implemented and ready for unit testing)
- Story 4.1: End-to-End Integration (integration points identified for testing)

## Definition of Done
- All acceptance criteria are met and validated
- Unit test coverage >90% for all critical code paths
- Test framework is documented and team is trained
- CI/CD pipeline runs tests automatically
- Test execution time is optimized (<5 minutes for full suite)
- All tests are passing and maintainable

## Testing Requirements

### Unit Tests
- **AI Services**: Query generation, contact extraction, search orchestration
- **API Controllers**: All AI search endpoints with various scenarios
- **Database Operations**: Repository patterns, data access layers
- **Utility Functions**: Data transformation, validation, formatting
- **Error Handling**: All error scenarios and recovery mechanisms
- **External API Integration**: Mock implementations and error handling

### Test Categories
- **Happy Path Tests**: Normal operation scenarios
- **Error Scenarios**: API failures, timeouts, invalid responses
- **Edge Cases**: Empty results, malformed data, boundary conditions
- **Performance Tests**: Response times, memory usage, concurrent operations
- **Security Tests**: Input validation, authentication, authorization

### Test Data Management
- **Fixtures**: Predefined test data for various scenarios
- **Factories**: Dynamic test data generation
- **Seeds**: Database seeding for consistent test environments
- **Cleanup**: Proper test isolation and cleanup procedures

## Implementation Notes

### Current Testing Infrastructure Analysis

Based on the existing codebase analysis, the project already has a comprehensive testing foundation:

#### Existing Jest Configurations
- **General Jest Config** (`config/jest.config.js`): Basic setup for frontend testing with jsdom environment
- **AI-Specific Config** (`config/jest.ai.config.js`): Advanced configuration for AI service testing with:
  - Node environment for backend testing
  - 90% coverage thresholds (95% for critical services)
  - Multiple coverage reporters (text, HTML, lcov, JSON)
  - 30-second timeout for AI operations
  - Global setup/teardown for test isolation

#### Existing Test Infrastructure
- **AI Test Setup** (`src/__tests__/setup/ai-test-setup.ts`): Comprehensive mocking for:
  - OpenAI and Anthropic AI services
  - Redis caching layer
  - HTTP requests (axios, fetch)
  - Environment variables and test utilities
  - Global test helpers for creating mock data

#### Current Test Coverage
- **17 existing AI test files** covering:
  - AI services configuration and integration
  - Query generation (template engine, scoring, deduplication, AI enhancement)
  - Contact extraction (content parsing, confidence scoring, duplicate detection)
  - Search orchestration (caching, service coordination)
  - API controllers and shared middleware

### Development Guidelines
- **Follow existing patterns**: The codebase already demonstrates excellent testing practices
- **Use descriptive test names**: Current tests follow the pattern "should [behavior] when [condition]"
- **Maintain test isolation**: Current setup provides proper cleanup and mock management
- **Document mock behavior**: Existing mocks are well-documented and realistic
- **Keep tests maintainable**: Current structure shows good separation of concerns

### Best Practices Already Implemented
- **Arrange-Act-Assert pattern**: All existing tests follow this structure
- **Comprehensive mocking**: External services are properly mocked with realistic behavior
- **Test data management**: Global utilities provide consistent mock data creation
- **Behavior-focused testing**: Tests validate behavior rather than implementation details
- **TDD approach evidence**: Tests are structured alongside production code

### Enhanced Test Structure Examples

#### Service Testing Pattern
```typescript
describe('QueryGenerationService', () => {
  describe('generateQueries', () => {
    it('should generate relevant queries for media contact search', async () => {
      // Arrange
      const request = global.testUtils.createMockRequest({
        criteria: { country: 'US', category: 'technology' }
      });

      // Act
      const result = await service.generateQueries(request);

      // Assert
      expect(result.queries).toHaveLength.greaterThan(0);
      expect(result.queries[0].query).toContain('technology');
      expect(result.metrics.coverageByCriteria.categories).toContain('Technology');
    });
  });
});
```

#### API Controller Testing Pattern
```typescript
describe('AI Search Controller', () => {
  describe('POST /api/ai/search', () => {
    it('should return search results for valid request', async () => {
      // Arrange
      const mockRequest = {
        method: 'POST',
        body: { query: 'AI journalists', criteria: { categories: ['Technology'] } }
      };

      // Act
      const response = await POST(mockRequest);

      // Assert
      expect(response.status).toBe(200);
      const data = await response.json();
      expect(data.results).toBeDefined();
      expect(data.queryId).toBeDefined();
    });
  });
});
```

### Mock Strategy Enhancement Opportunities

#### External API Mocks
- **OpenAI/Anthropic**: Already implemented with realistic response patterns
- **Exa Search**: Add comprehensive mock for search result scenarios
- **Firecrawl**: Add content extraction mock with various content types
- **Rate Limiting**: Add mock for rate limiting scenarios

#### Database Testing Strategy
- **Test Database**: Use existing test database configuration with proper seeding
- **Transaction Mocking**: Enhance existing Prisma mocks for complex operations
- **Data Consistency**: Add tests for concurrent database operations

#### Time and Network Mocking
- **Time-dependent operations**: Add jest.useFakeTimers for consistent testing
- **Network resilience**: Add mock for network failure scenarios
- **Timeout handling**: Add tests for operation timeout scenarios

### Performance Testing Framework

#### Execution Time Monitoring
```typescript
describe('Performance Tests', () => {
  it('should complete query generation within acceptable time', async () => {
    const startTime = performance.now();

    await service.generateQueries(testRequest);

    const executionTime = performance.now() - startTime;
    expect(executionTime).toBeLessThan(5000); // 5 second limit
  });
});
```

#### Memory Usage Testing
```typescript
describe('Memory Usage Tests', () => {
  it('should maintain reasonable memory usage during batch operations', async () => {
    const initialMemory = process.memoryUsage().heapUsed;

    // Perform batch operation
    await service.processBatch(largeRequest);

    const finalMemory = process.memoryUsage().heapUsed;
    const memoryIncrease = finalMemory - initialMemory;

    expect(memoryIncrease).toBeLessThan(50 * 1024 * 1024); // 50MB limit
  });
});
```

### Test Data Management Strategy

#### Fixtures and Factories
- **Existing utilities**: Enhance global.testUtils with more comprehensive fixtures
- **Dynamic factories**: Add factories for complex test scenarios
- **Edge case data**: Add specific fixtures for error scenarios

#### Test Seeding Strategy
- **Database seeding**: Enhance existing seed scripts for comprehensive test data
- **Cleanup procedures**: Ensure proper test isolation with database cleanup
- **Consistent environments**: Use Docker or similar for consistent test environments

### Enhanced Mock Strategy

#### AI Service Response Mocking
```typescript
// Enhanced OpenAI mock for different scenarios
const createOpenAIMock = (scenario: 'success' | 'rate_limit' | 'error') => {
  switch (scenario) {
    case 'success':
      return {
        choices: [{
          message: { content: 'Generated AI response' }
        }]
      };
    case 'rate_limit':
      throw new Error('Rate limit exceeded');
    case 'error':
      throw new Error('AI service unavailable');
  }
};
```

#### Database Transaction Mocking
```typescript
// Enhanced Prisma transaction mock
const mockTransaction = (callback: any) => {
  const mockTx = {
    ai_generated_queries: { create: jest.fn() },
    ai_query_performance_logs: { create: jest.fn() },
    // ... other models
  };
  return callback(mockTx);
};
```

### Performance Optimization Strategies

#### Parallel Test Execution
- **Worker optimization**: Configure maxWorkers for optimal performance
- **Test isolation**: Ensure tests can run in parallel without conflicts
- **Resource management**: Proper cleanup of shared resources

#### Mock Optimization
- **Lightweight mocks**: Use simple mocks for performance-critical tests
- **Realistic mocks**: Use comprehensive mocks for integration tests
- **Conditional mocking**: Apply different mock strategies based on test type

#### Coverage Optimization
- **Threshold balancing**: Adjust coverage thresholds based on criticality
- **Exclusion patterns**: Exclude non-critical code from coverage requirements
- **Focused testing**: Prioritize high-impact code paths

### Integration with CI/CD Pipeline

#### Automated Testing Workflow
```yaml
# Enhanced GitHub Actions workflow
name: AI Testing Pipeline
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
      - run: npm ci
      - run: npm run test:ai:coverage
      - uses: codecov/codecov-action@v3
```

#### Performance Benchmarking
- **Baseline metrics**: Establish performance baselines for AI operations
- **Regression detection**: Alert on performance degradation
- **Trend analysis**: Track performance trends over time

### Test Documentation Strategy

#### Living Documentation
- **Test examples**: Maintain examples of common testing patterns
- **Mock documentation**: Document mock behaviors and scenarios
- **Best practices**: Keep testing guidelines updated with project evolution

#### Developer Onboarding
- **Testing patterns**: Provide clear examples for new developers
- **Mock usage**: Document how to use and extend existing mocks
- **Coverage guidelines**: Explain coverage requirements and strategies

## Implementation Tasks

### Phase 1: Framework Enhancement (Day 1)

#### Task 1.1: Validate and Optimize Existing Configuration
- **Current State Assessment**: Review existing Jest configurations
  - Validate `config/jest.ai.config.js` meets all requirements
  - Check coverage thresholds are appropriate (90%/95% targets)
  - Verify test timeout settings (30 seconds for AI operations)
  - Test global setup/teardown functionality

- **Performance Optimization**: Enhance test execution speed
  - Optimize maxWorkers configuration for parallel execution
  - Review test patterns for potential bottlenecks
  - Implement selective test execution for development
  - Add test performance monitoring

#### Task 1.2: Expand AI Service Mock Library
- **Missing Service Mocks**: Complete external service mocking
  - Add comprehensive Exa Search API mocks with various response scenarios
  - Implement Firecrawl content extraction mocks with different content types
  - Create rate limiting scenario mocks for all AI services
  - Add timeout and network error simulation utilities

- **Enhanced Mock Behaviors**: Improve mock realism
  ```typescript
  // Example: Enhanced Exa Search mock
  const createExaSearchMock = (scenario: 'success' | 'no_results' | 'rate_limit' | 'error') => {
    switch (scenario) {
      case 'success':
        return {
          results: [
            { title: 'AI Journalist Profile', url: 'https://example.com', score: 0.95 },
            { title: 'Tech Media Contact', url: 'https://example2.com', score: 0.87 }
          ],
          numResults: 2,
          totalResults: 245
        };
      case 'no_results':
        return { results: [], numResults: 0, totalResults: 0 };
      case 'rate_limit':
        throw new Error('Rate limit exceeded. Try again later.');
      case 'error':
        throw new Error('Search service temporarily unavailable');
    }
  };
  ```

#### Task 1.3: Implement Missing Unit Tests
- **API Controller Tests**: Complete endpoint coverage
  - `POST /api/ai/search`: Comprehensive request/response testing
  - `POST /api/ai/query-generation`: Various criteria and scenarios
  - `POST /api/ai/contact-extraction`: Different content types and edge cases
  - `GET /api/ai/status`: Service health and configuration checks

- **Database Repository Tests**: Data layer validation
  - AI query templates CRUD operations
  - Generated queries storage and retrieval
  - Performance logging and metrics
  - Transaction handling and rollback scenarios

- **Utility Function Tests**: Helper function validation
  - Data transformation utilities
  - Validation and formatting functions
  - Configuration parsing and validation
  - Error handling and logging utilities

### Phase 2: Advanced Testing Framework (Day 1-2)

#### Task 2.1: Performance Testing Implementation
- **Execution Time Monitoring**: Response time validation
  ```typescript
  describe('Query Generation Performance', () => {
    it('should complete within acceptable time limits', async () => {
      const startTime = performance.now();

      await service.generateQueries(testRequest);

      const executionTime = performance.now() - startTime;
      expect(executionTime).toBeLessThan(5000); // 5 second threshold

      // Log performance for monitoring
      console.log(`Query generation completed in ${executionTime.toFixed(2)}ms`);
    });
  });
  ```

- **Memory Usage Testing**: Resource consumption monitoring
  ```typescript
  describe('Memory Usage Tests', () => {
    it('should maintain reasonable memory usage during batch operations', async () => {
      const initialMemory = process.memoryUsage().heapUsed;

      // Process large batch of requests
      await service.processBatch(largeBatchRequest);

      const finalMemory = process.memoryUsage().heapUsed;
      const memoryIncrease = finalMemory - initialMemory;

      expect(memoryIncrease).toBeLessThan(50 * 1024 * 1024); // 50MB limit
    });
  });
  ```

- **Concurrent Operation Testing**: Stress testing
  ```typescript
  describe('Concurrent Request Handling', () => {
    it('should handle multiple simultaneous requests', async () => {
      const concurrentRequests = Array(10).fill(null).map((_, i) =>
        service.generateQueries(createMockRequest({ batchId: `batch-${i}` }))
      );

      const results = await Promise.all(concurrentRequests);

      expect(results).toHaveLength(10);
      results.forEach(result => {
        expect(result.status).toBe('completed');
        expect(result.queries.length).toBeGreaterThan(0);
      });
    });
  });
  ```

#### Task 2.2: Test Data Management Enhancement
- **Enhanced Test Utilities**: Expand global.testUtils
  ```typescript
  // Add to src/__tests__/setup/ai-test-setup.ts
  global.testUtils = {
    ...global.testUtils,

    createComplexMockRequest: (overrides = {}) => ({
      ...createMockRequest(),
      criteria: {
        categories: ['Technology', 'Business'],
        beats: ['AI', 'Startups'],
        countries: ['US', 'GB', 'CA'],
        languages: ['English'],
        topics: ['artificial intelligence', 'machine learning'],
        outlets: ['TechCrunch', 'Wired', 'VentureBeat'],
        ...overrides.criteria
      },
      options: {
        maxQueries: 15,
        diversityThreshold: 0.75,
        minRelevanceScore: 0.4,
        enableAIEnhancement: true,
        fallbackStrategies: true,
        cacheEnabled: true,
        priority: 'high',
        ...overrides.options
      }
    }),

    createBatchRequests: (count = 5, baseOverrides = {}) =>
      Array(count).fill(null).map((_, i) =>
        createComplexMockRequest({
          ...baseOverrides,
          batchId: `batch-${i}`,
          searchId: `search-${i}`
        })
      ),

    createMockSearchResults: (count = 10) =>
      Array(count).fill(null).map((_, i) => ({
        id: `result-${i}`,
        title: `AI Technology Contact ${i}`,
        url: `https://example.com/contact-${i}`,
        confidence: 0.8 + Math.random() * 0.2,
        metadata: {
          source: 'exa_search',
          queryId: `query-${i}`,
          rank: i + 1
        }
      }))
  };
  ```

### Phase 3: Integration and Quality Assurance (Day 2)

#### Task 3.1: End-to-End Integration Tests
- **Complete Request Flow Testing**: API to Database
  ```typescript
  describe('AI Search Integration', () => {
    it('should process complete search request from API to database', async () => {
      // Arrange: Setup complete test environment
      await testSeeder.seedQueryTemplates();

      const requestBody = {
        query: 'AI technology journalists',
        criteria: {
          categories: ['Technology'],
          countries: ['US']
        },
        options: {
          maxQueries: 10,
          enableAIEnhancement: true
        }
      };

      // Act: Execute API request
      const response = await POST({
        method: 'POST',
        body: requestBody,
        headers: { 'Content-Type': 'application/json' }
      });

      // Assert: Validate complete flow
      expect(response.status).toBe(200);
      const data = await response.json();

      expect(data.searchId).toBeDefined();
      expect(data.results).toBeDefined();
      expect(data.results.length).toBeGreaterThan(0);

      // Verify database records
      const savedQueries = await prisma.ai_generated_queries.findMany({
        where: { searchId: data.searchId }
      });
      expect(savedQueries.length).toBeGreaterThan(0);
    });
  });
  ```

#### Task 3.2: CI/CD Pipeline Integration
- **GitHub Actions Workflow**: Automated testing
  ```yaml
  # .github/workflows/ai-testing.yml
  name: AI Services Testing Pipeline

  on:
    push:
      branches: [main, epic-2-ai-search-service-complete]
    pull_request:
      branches: [main]

  jobs:
    test-ai-services:
      runs-on: ubuntu-latest
      steps:
        - uses: actions/checkout@v4
        - name: Setup Node.js
          uses: actions/setup-node@v4
          with:
            node-version: '20'
            cache: 'npm'
        - name: Install dependencies
          run: npm ci
        - name: Run AI unit tests
          run: npm run test:ai:coverage
        - name: Upload coverage to Codecov
          uses: codecov/codecov-action@v3
  ```

## Related Documentation
- [Epic Definition](../epics/epic-4-integration-testing.mdc)
- [Implementation Tasks - Task 4.2](../implementation-tasks.mdc#task-42-comprehensive-testing-suite)
- [Jest Documentation](https://jestjs.io/docs/getting-started)
- [Testing Best Practices](../../../docs/testing/best-practices.md)
- [Existing Test Patterns](../../../docs/testing/patterns.md)
- [CI/CD Configuration](../../../docs/ci-cd/testing.md)